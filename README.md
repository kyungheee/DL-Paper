# DL-Paper-Implementations


## Convolutional Neural Network

| Paper Title | Description | GitHub Link |
|-------------|-------------|-------------|
| [AlexNet: Deep learning model for ImageNet classification](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) | Deep learning model for ImageNet classification | [GitHub](https://github.com/pytorch/vision/blob/main/torchvision/models/alexnet.py) |
| [VGGNet: Very deep convolutional networks](https://arxiv.org/abs/1409.1556) | Very deep convolutional networks | [GitHub](https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py) |
| [ResNet: Deep residual learning framework](https://arxiv.org/abs/1512.03385) | Deep residual learning framework | [GitHub](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py) |

## Recurrent Neural Network

| Paper Title | Description | GitHub Link |
|-------------|-------------|-------------|
| [LSTM: Long short-term memory](https://www.bioinf.jku.at/publications/older/2604.pdf) | Long short-term memory | [GitHub](https://github.com/pytorch/examples/tree/main/time_sequence_prediction) |
| [GRU: Gated recurrent unit](https://arxiv.org/abs/1406.1078) | Gated recurrent unit | [GitHub](https://github.com/pytorch/examples/tree/main/time_sequence_prediction) |
| [Attention Mechanism: Neural machine translation by jointly learning to align and translate](https://arxiv.org/abs/1409.0473) | Neural machine translation by jointly learning to align and translate | [GitHub](https://github.com/jadore801120/attention-is-all-you-need-pytorch) |

## Attention Mechanism

| Paper Title | Description | GitHub Link |
|-------------|-------------|-------------|
| [Transformer: Attention is all you need](https://arxiv.org/abs/1706.03762) | Attention is all you need | [GitHub](https://github.com/jadore801120/attention-is-all-you-need-pytorch) |
| [BERT: Pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/abs/1810.04805) | Pre-training of deep bidirectional transformers for language understanding | [GitHub](https://github.com/huggingface/transformers) |
| [GPT-3: Language models are few-shot learners](https://arxiv.org/abs/2005.14165) | Language models are few-shot learners | [GitHub](https://github.com/karpathy/minGPT) |

## Auto-Encoder & VAE

| Paper Title | Description | GitHub Link |
|-------------|-------------|-------------|
| [Variational Autoencoder (VAE): Probabilistic model for generating data](https://arxiv.org/abs/1312.6114) | Probabilistic model for generating data | [GitHub](https://github.com/AntixK/PyTorch-VAE) |
| [Denoising Autoencoder (DAE): Autoencoder for noise reduction](https://www.cs.toronto.edu/~hinton/absps/NC2006.pdf) | Autoencoder for noise reduction | [GitHub](https://github.com/AgustinusKristiadi/denoising-autoencoder) |
| [Sparse Autoencoder: Autoencoder with sparsity constraint](https://cs.stanford.edu/~quocle/LeKarpenkoNgiamCoatesICML2011.pdf) | Autoencoder with sparsity constraint | [GitHub](https://github.com/naokishibuya/deep-learning-zero-to-all) |

## Generative Adversarial Network

| Paper Title | Description | GitHub Link |
|-------------|-------------|-------------|
| [GAN: Generative adversarial networks](https://arxiv.org/abs/1406.2661) | Generative adversarial networks | [GitHub](https://github.com/eriklindernoren/PyTorch-GAN) |
| [DCGAN: Deep convolutional GANs](https://arxiv.org/abs/1511.06434) | Deep convolutional GANs | [GitHub](https://github.com/eriklindernoren/PyTorch-GAN) |
| [CycleGAN: Image-to-image translation with GANs](https://arxiv.org/abs/1703.10593) | Image-to-image translation with GANs | [GitHub](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) |

## Natural Language Processing

| Paper Title | Description | GitHub Link |
|-------------|-------------|-------------|
| [BERT: Transformer-based model for NLP](https://arxiv.org/abs/1810.04805) | Transformer-based model for NLP | [GitHub](https://github.com/huggingface/transformers) |
| [GPT-3: Generative pre-trained transformer model](https://arxiv.org/abs/2005.14165) | Generative pre-trained transformer model | [GitHub](https://github.com/karpathy/minGPT) |
| [Transformer: Attention mechanism for sequence modeling](https://arxiv.org/abs/1706.03762) | Attention mechanism for sequence modeling | [GitHub](https://github.com/jadore801120/attention-is-all-you-need-pytorch) |

## Graph Neural Network

| Paper Title | Description | GitHub Link |
|-------------|-------------|-------------|
| [GCN: Graph convolutional networks](https://arxiv.org/abs/1609.02907) | Graph convolutional networks | [GitHub](https://github.com/tkipf/pygcn) |
| [GraphSAGE: Inductive representation learning on large graphs](https://arxiv.org/abs/1706.02216) | Inductive representation learning on large graphs | [GitHub](https://github.com/williamleif/GraphSAGE) |
| [GAT: Graph attention networks](https://arxiv.org/abs/1710.10903) | Graph attention networks | [GitHub](https://github.com/Diego999/pyGAT) |
